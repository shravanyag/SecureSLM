{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class PreProcess:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.df = pd.read_csv(file_path, encoding='utf-8', engine='python')\n",
    "        self.cleaned_text = \"\"\n",
    "\n",
    "    def handle_float_nans(self):\n",
    "        float_columns = ['EventId', 'ProcessId', 'ParentProcessId']\n",
    "        for col in float_columns:\n",
    "            if col in self.df.columns:\n",
    "                self.df[col] = self.df[col].fillna(-1)\n",
    "\n",
    "    def remove_nans(self):\n",
    "        self.df.fillna('', inplace=True)\n",
    "\n",
    "    def normalize_text(self):\n",
    "        def normalize(value):\n",
    "            return str(value).replace(',', ' ').replace('\\n', ' ').replace('\\r', '').strip()\n",
    "        for col in self.df.select_dtypes(include='object').columns:\n",
    "            self.df[col] = self.df[col].map(normalize)\n",
    "\n",
    "\n",
    "    def select_columns(self):\n",
    "        columns_to_keep = [\n",
    "            'TimeCreated', 'EventId', 'ProcessId', 'ParentProcessId',\n",
    "            'Image', 'CommandLine', 'ParentImage', 'ParentCommandLine',\n",
    "            'CurrentDirectory', 'User', 'IntegrityLevel'\n",
    "        ]\n",
    "        self.df = self.df[columns_to_keep]\n",
    "\n",
    "    def rows_to_paragraphs(self):\n",
    "        def row_to_paragraph(row):\n",
    "\n",
    "            return (\n",
    "                f\"On {row['TimeCreated']}, Event ID {row['EventId']} occurred. \"\n",
    "                f\"Process '{row['Image']}' (PID: {row['ProcessId']}) was launched using the command: {row['CommandLine']}. \"\n",
    "                f\"It was spawned by '{row['ParentImage']}' (PID: {row['ParentProcessId']}) with command: {row['ParentCommandLine']}. \"\n",
    "                f\"The process ran in directory '{row['CurrentDirectory']}' under user '{row['User']}' \"\n",
    "                f\"with an integrity level of '{row['IntegrityLevel']}'.\"\n",
    "            )\n",
    "        self.df['log_paragraph'] = self.df.apply(row_to_paragraph, axis=1)\n",
    "\n",
    "    def create_megastring(self):\n",
    "        self.cleaned_text = '\\n'.join(self.df['log_paragraph'].tolist())\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        self.handle_float_nans()\n",
    "        self.remove_nans()\n",
    "        self.normalize_text()\n",
    "        self.select_columns()\n",
    "        self.rows_to_paragraphs()\n",
    "        self.create_megastring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "class SyntheticData:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.cleaned_text = \"\"\n",
    "        self.response_text = \"\"\n",
    "\n",
    "    def generate_cleaned_text(self):\n",
    "        log_processor = PreProcess(self.file_path)\n",
    "        log_processor.run_pipeline()\n",
    "        self.cleaned_text = log_processor.cleaned_text\n",
    "\n",
    "    def generate_response(self, api_key, endpoint, deployment_name):\n",
    "        prompt_prefix = (\n",
    "            \"As a security analyst, you are presented with raw Windows Sysmon logs from a red team simulation.\"\n",
    "            \"Instructions: \"\n",
    "            \"- Interpret each log as if investigating a potential compromise.\"\n",
    "            \"- Call out behaviors suggestive of lateral movement, execution from temporary directories, PowerShell abuse, file obfuscation, etc.\"\n",
    "            \"- Be assertive in flagging anything that seems out of place.\"\n",
    "            \"- Prioritize security over ambiguityâ€”it's okay to over-classify in favor of catching threats.\"\n",
    "            \"List each entry as:\"\n",
    "            \"- Summary\"\n",
    "            \"- Suspicion Level: Benign / Suspicious / Malicious\"\n",
    "            \"- Explanation\"\n",
    "\n",
    "            \"Begin analysis below:\"\n",
    "        )\n",
    "        full_prompt = prompt_prefix + self.cleaned_text\n",
    "        \n",
    "        headers = {\n",
    "            \"api-key\": api_key,\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        url = f\"{endpoint}openai/deployments/{deployment_name}/chat/completions?api-version=2024-03-01-preview\"\n",
    "        payload = {\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": full_prompt}],\n",
    "            \"temperature\": 0.6,\n",
    "            \"top_p\": 0.95\n",
    "        }\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        response_json = response.json()\n",
    "        self.response_text = response_json['choices'][0]['message']['content']\n",
    "\n",
    "    def save_to_csv(self, output_path, append=False):\n",
    "        import pandas as pd\n",
    "        import os\n",
    "\n",
    "        df = pd.DataFrame([{\n",
    "            'input_text': self.cleaned_text,\n",
    "            'copilot_response': self.response_text\n",
    "        }])\n",
    "\n",
    "        if append and os.path.exists(output_path):\n",
    "            df.to_csv(output_path, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class SyntheticBatchProcessor:\n",
    "    def __init__(self, folder_path, output_path, api_key, endpoint, deployment_name):\n",
    "        self.folder_path = folder_path\n",
    "        self.output_path = output_path\n",
    "        self.api_key = api_key\n",
    "        self.endpoint = endpoint\n",
    "        self.deployment_name = deployment_name\n",
    "\n",
    "    def process_all_files(self):\n",
    "        # Loop over all folders inside the parent folder\n",
    "        for child_folder in os.listdir(self.folder_path):\n",
    "            child_path = os.path.join(self.folder_path, child_folder)\n",
    "            \n",
    "            # Proceed only if it's a directory\n",
    "            if os.path.isdir(child_path):\n",
    "                for filename in os.listdir(child_path):\n",
    "                    if filename.endswith(\".csv\"):\n",
    "                        file_path = os.path.join(child_path, filename)\n",
    "                        \n",
    "                        print(f\"ðŸ” Processing: {filename} in {child_folder}\")\n",
    "                        sd = SyntheticData(file_path)\n",
    "                        sd.generate_cleaned_text()\n",
    "                        sd.generate_response(\n",
    "                            api_key=self.api_key,\n",
    "                            endpoint=self.endpoint,\n",
    "                            deployment_name=self.deployment_name\n",
    "                        )\n",
    "                        sd.save_to_csv(self.output_path, append=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Processing: T1001.002-1.csv\n",
      "ðŸ” Processing: T1001.002-2.csv\n",
      "ðŸ” Processing: T1003-1.csv\n",
      "ðŸ” Processing: T1003-2.csv\n",
      "ðŸ” Processing: T1003-3.csv\n",
      "ðŸ” Processing: T1003-4.csv\n",
      "ðŸ” Processing: T1003-5.csv\n",
      "ðŸ” Processing: T1003-6.csv\n",
      "ðŸ” Processing: T1003.001-1.csv\n",
      "ðŸ” Processing: T1003.001-10.csv\n",
      "ðŸ” Processing: T1003.001-11.csv\n",
      "ðŸ” Processing: T1003.001-12.csv\n",
      "ðŸ” Processing: T1003.001-13.csv\n",
      "ðŸ” Processing: T1003.001-14.csv\n",
      "ðŸ” Processing: T1003.001-2.csv\n",
      "ðŸ” Processing: T1003.001-3.csv\n",
      "ðŸ” Processing: T1003.001-4.csv\n",
      "ðŸ” Processing: T1003.001-6.csv\n",
      "ðŸ” Processing: T1003.001-8.csv\n",
      "ðŸ” Processing: T1003.001-9.csv\n",
      "ðŸ” Processing: T1003.002-1.csv\n",
      "ðŸ” Processing: T1003.002-3.csv\n",
      "ðŸ” Processing: T1003.002-4.csv\n",
      "ðŸ” Processing: T1003.002-5.csv\n",
      "ðŸ” Processing: T1003.002-6.csv\n",
      "ðŸ” Processing: T1003.002-8.csv\n",
      "ðŸ” Processing: T1003.003-2.csv\n",
      "ðŸ” Processing: T1003.003-4.csv\n",
      "ðŸ” Processing: T1003.003-5.csv\n",
      "ðŸ” Processing: T1003.003-6.csv\n",
      "ðŸ” Processing: T1003.003-7.csv\n",
      "ðŸ” Processing: T1003.004-2.csv\n",
      "ðŸ” Processing: T1003.005-1.csv\n",
      "ðŸ” Processing: T1003.006-1.csv\n",
      "ðŸ” Processing: T1003.006-2.csv\n",
      "ðŸ” Processing: T1005-1.csv\n",
      "ðŸ” Processing: T1006-1.csv\n",
      "ðŸ” Processing: T1007-1.csv\n",
      "ðŸ” Processing: T1007-4.csv\n",
      "ðŸ” Processing: T1010-1.csv\n",
      "ðŸ” Processing: T1012-1.csv\n",
      "ðŸ” Processing: T1012-2.csv\n",
      "ðŸ” Processing: T1012-3.csv\n",
      "ðŸ” Processing: T1012-6.csv\n",
      "ðŸ” Processing: T1016-1.csv\n",
      "ðŸ” Processing: T1016-2.csv\n",
      "ðŸ” Processing: T1016-4.csv\n",
      "ðŸ” Processing: T1016-5.csv\n",
      "ðŸ” Processing: T1016-7.csv\n",
      "ðŸ” Processing: T1016-9.csv\n",
      "ðŸ” Processing: T1016.001-1.csv\n",
      "ðŸ” Processing: T1016.001-3.csv\n",
      "ðŸ” Processing: T1016.001-4.csv\n",
      "ðŸ” Processing: T1016.001-5.csv\n",
      "ðŸ” Processing: T1018-1.csv\n",
      "ðŸ” Processing: T1018-10.csv\n",
      "ðŸ” Processing: T1018-11.csv\n",
      "ðŸ” Processing: T1018-16.csv\n",
      "ðŸ” Processing: T1018-17.csv\n",
      "ðŸ” Processing: T1018-18.csv\n",
      "ðŸ” Processing: T1018-19.csv\n",
      "ðŸ” Processing: T1018-2.csv\n",
      "ðŸ” Processing: T1018-20.csv\n",
      "ðŸ” Processing: T1018-22.csv\n",
      "ðŸ” Processing: T1018-3.csv\n",
      "ðŸ” Processing: T1018-4.csv\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'choices'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m processor = SyntheticBatchProcessor(\n\u001b[32m      2\u001b[39m     folder_path=\u001b[33m\"\u001b[39m\u001b[33mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mshrav\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mDocuments\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mMTECH\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mSem IV\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mSecureSLM\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mData\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mprompt3\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     output_path=\u001b[33m\"\u001b[39m\u001b[33mcopilot_output_3.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m     deployment_name=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_all_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mSyntheticBatchProcessor.process_all_files\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     17\u001b[39m sd = SyntheticData(file_path)\n\u001b[32m     18\u001b[39m sd.generate_cleaned_text()\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43msd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdeployment_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdeployment_name\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m sd.save_to_csv(\u001b[38;5;28mself\u001b[39m.output_path, append=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mSyntheticData.generate_response\u001b[39m\u001b[34m(self, api_key, endpoint, deployment_name)\u001b[39m\n\u001b[32m     41\u001b[39m response = requests.post(url, headers=headers, json=payload)\n\u001b[32m     42\u001b[39m response_json = response.json()\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28mself\u001b[39m.response_text = \u001b[43mresponse_json\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mchoices\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mKeyError\u001b[39m: 'choices'"
     ]
    }
   ],
   "source": [
    "processor = SyntheticBatchProcessor(\n",
    "    folder_path=\"C:\\\\Users\\\\shrav\\\\Documents\\\\MTECH\\\\Sem IV\\\\SecureSLM\\\\Data\\\\new\",\n",
    "    output_path=\"copilot_output_3.csv\",\n",
    "    api_key=\"M2tMQ5DGXcyJNY45FQS8syNc9RdDtgSCjPic9Tdb0DYh6AVZCd93JQQJ99BGACHYHv6XJ3w3AAAAACOGpz6C\",\n",
    "    endpoint=\"https://shrav-mckii6fb-eastus2.cognitiveservices.azure.com/\",\n",
    "    deployment_name=\"gpt-4o\"\n",
    ")\n",
    "\n",
    "processor.process_all_files()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
