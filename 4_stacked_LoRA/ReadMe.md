Only the tinyLlaMA has been implemented for stacked LoRA. The wrong file represents what can go wrong then the stacking order is upset.
